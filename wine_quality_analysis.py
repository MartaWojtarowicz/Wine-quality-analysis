# -*- coding: utf-8 -*-
"""Wine quality analysis.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1s-lXRthBzi_ulg5EpXRZO5Fi4DE2NqQj

#Load dataset

https://archive.ics.uci.edu/dataset/186/wine+quality \
The two datasets are related to red and white variants of the Portuguese "Vinho Verde" wine. There is no missing values.\
Vinho Verde is a young Portuguese wine, typically white, but can also be red or rosé. It is made from a variety of local grapes, including Alvarinho, Loureiro, and Arinto. Known for its high acidity, crispness, and sometimes a slight effervescence.
"""

import pandas as pd

white_wine = pd.read_csv('winequality-white.csv', delimiter=';')
red_wine = pd.read_csv('winequality-red.csv', delimiter=';')

white_wine2 = pd.read_csv('winequality-white.csv', delimiter=';')
red_wine2 = pd.read_csv('winequality-red.csv', delimiter=';')

white_wine.info()

desc = white_wine[['fixed acidity', 'volatile acidity', 'citric acid', 'residual sugar', 'chlorides',
 'free sulfur dioxide', 'total sulfur dioxide', 'density', 'pH', 'sulphates', 'alcohol', 'quality']
].describe().rename(index={
    'count': 'Total Entries',
    'mean': 'Average Value',
    'std': 'Standard Deviation',
    'min': 'Minimum Value',
    '25%': 'First Quartile (Q1)',
    '50%': 'Median (Q2)',
    '75%': 'Third Quartile (Q3)',
    'max': 'Maximum Value'
}).round(3)

display(desc)

red_wine.info()

desc = red_wine[['fixed acidity', 'volatile acidity', 'citric acid', 'residual sugar', 'chlorides',
 'free sulfur dioxide', 'total sulfur dioxide', 'density', 'pH', 'sulphates', 'alcohol', 'quality']
].describe().rename(index={
    'count': 'Total Entries',
    'mean': 'Average Value',
    'std': 'Standard Deviation',
    'min': 'Minimum Value',
    '25%': 'First Quartile (Q1)',
    '50%': 'Median (Q2)',
    '75%': 'Third Quartile (Q3)',
    'max': 'Maximum Value'
}).round(3)

display(desc)

"""# pH and alocohol distribiution plots"""

import seaborn as sns
import matplotlib.pyplot as plt

plt.figure(figsize=(8, 5))

sns.kdeplot(red_wine['pH'], color='red', label='Red Wine', fill=True)
sns.kdeplot(white_wine['pH'], color='green', label='White Wine', fill=True)

plt.xlabel('pH')
plt.ylabel('Density')
plt.title('pH Distribution for Red and White Wine')
plt.legend()
plt.show()

fig, axes = plt.subplots(1, 2, figsize=(12, 5))  # 1 row, 2 columns

# Red wine pH distribution
sns.histplot(red_wine['pH'], bins=30, color='red', kde=True, ax=axes[0])
axes[0].set_title('Red Wine pH Distribution')

# White wine pH distribution
sns.histplot(white_wine['pH'], bins=30, color='green', kde=True, ax=axes[1])
axes[1].set_title('White Wine pH Distribution')

plt.show()

plt.figure(figsize=(8, 5))

sns.kdeplot(red_wine['alcohol'], color='red', label='Red Wine', fill=True)
sns.kdeplot(white_wine['alcohol'], color='green', label='White Wine', fill=True)

plt.xlabel('Alcohol %')
plt.ylabel('Density')
plt.title('Alcohol Percentage Distribution for Red and White Wine')
plt.legend()
plt.show()

fig, axes = plt.subplots(1, 2, figsize=(12, 5))  # 1 row, 2 columns

# Red wine alcohol distribution
sns.histplot(red_wine['alcohol'], bins=30, color='red', kde=True, ax=axes[0])
axes[0].set_title('Red Wine Alcohol % Distribution')

# White wine alcohol distribution
sns.histplot(white_wine['alcohol'], bins=30, color='green', kde=True, ax=axes[1])
axes[1].set_title('White Wine Alcohol % Distribution')

plt.show()

"""Red wine is typically denser than white wine due to its higher levels of phenolics, tannins, and other compounds extracted from the grape skins. In white wine, the skins are removed before fermentation, resulting in a lighter, less tannic wine. This difference in winemaking processes also contributes to red wines generally having a higher alcohol content, as red grapes are often harvested riper, leading to more sugar for fermentation.\
White grapes are usually harvested earlier and fermented at lower temperatures. As a result, white wine tends to have a lower pH, making it more acidic. This higher acidity impacts the taste, giving white wines a fresher, crisper, and more vibrant profile, which is a desired outcome for this style of wine.

# Heatmap
"""

import seaborn as sns
import matplotlib.pyplot as plt

corr_matrix_red = red_wine.corr()
corr_matrix_white = white_wine.corr()


fig, axes = plt.subplots(1, 2, figsize=(18, 8))

sns.heatmap(corr_matrix_red, annot=True, cmap='coolwarm', fmt='.2f', linewidths=0.5, vmin=-1, vmax=1, ax=axes[0])
axes[0].set_title('Red Wine Correlation Heatmap')
sns.heatmap(corr_matrix_white, annot=True, cmap='coolwarm', fmt='.2f', linewidths=0.5, vmin=-1, vmax=1, ax=axes[1])
axes[1].set_title('White Wine Correlation Heatmap')
plt.tight_layout()
plt.show()

"""#Feature Importance for Wine Quality"""

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns


red_wine = red_wine.apply(pd.to_numeric, errors='coerce')
white_wine = white_wine.apply(pd.to_numeric, errors='coerce')

corr_red = red_wine.corr()['quality'].drop('quality').abs().sort_values(ascending=False)
corr_white = white_wine.corr()['quality'].drop('quality').abs().sort_values(ascending=False)
corr_df = pd.DataFrame({'Red Wine': corr_red, 'White Wine': corr_white})

corr_df['Max'] = corr_df.max(axis=1)
corr_df = corr_df.sort_values(by='Max', ascending=False).drop(columns=['Max'])


plt.figure(figsize=(10, 6))
ax = corr_df.plot(kind='barh', figsize=(12, 7), width=0.75, color=[sns.color_palette("Set1")[0], sns.color_palette("pastel")[0]])
plt.xlabel('Absolute Correlation with Quality', fontsize=14)
plt.ylabel('Feature', fontsize=14)
plt.title('Feature Importance for Wine Quality', fontsize=16, fontweight='bold')
plt.legend(title='Wine Type', fontsize=12)
plt.gca().invert_yaxis()
sns.despine()

plt.show()

"""The analysis shows that alcohol content is a key factor influencing wine quality, with higher alcohol levels generally associated with better quality for both red and white wines. However, alcohol content is carefully controlled during winemaking, as it is primarily determined by the sugar levels in the grapes. While higher alcohol can be achieved, it is not always desirable, as excessive alcohol can overpower other flavors and make the wine unbalanced. In red wines, volatile acidity has a moderate impact on quality, while its influence is weaker in white wines. Density is slightly more important for white wine quality than red, and other factors like sulphates and citric acid have minimal influence. Overall, alcohol content is a significant driver of quality, but the balance of all elements determines the final wine experience.

# **SMOTE**
"""

white_wine['quality'].value_counts().sort_index()

"""SMOTE (Synthetic Minority Over-sampling Technique) is an oversampling technique used to address class imbalance by generating synthetic examples for the minority class. This helps improve the model's ability to learn from underrepresented classes. The number of artificial samples should not exceed 10 times the number of existing samples."""

from imblearn.over_sampling import SMOTE
from sklearn.model_selection import train_test_split
import pandas as pd

# Split features and target
X_white = white_wine.drop(columns=['quality'])
y_white = white_wine['quality']

# Train-test split (adjust test size if needed)
X_train, X_test, y_train, y_test = train_test_split(X_white, y_white, test_size=0.2, random_state=999, stratify=y_white)

# Define minimum target per class
safe_minimum = 60

# Create sampling strategy
sampling_strategy = {}
class_counts = y_train.value_counts()

for label, count in class_counts.items():
    if count < safe_minimum:
        sampling_strategy[label] = safe_minimum
    else:
        sampling_strategy[label] = count

# Apply SMOTE to training data
smote = SMOTE(sampling_strategy=sampling_strategy, random_state=42, k_neighbors=2)
X_resampled, y_resampled = smote.fit_resample(X_train, y_train)

# Display new class distribution
print("After SMOTE:\n", pd.Series(y_resampled).value_counts())

"""#Random Forest

Random Forest is an ensemble learning method that builds multiple decision trees and aggregates their outputs to improve accuracy and reduce overfitting. It performs well with high-dimensional data and can handle imbalanced datasets more effectively than single classifiers. The number of trees (n_estimators) should be tuned for performance and efficiency.
"""

from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report

# Train the model on the resampled data
rf_model = RandomForestClassifier(n_estimators=100, random_state=999)
rf_model.fit(X_resampled, y_resampled)

# Evaluate on original test set
y_pred = rf_model.predict(X_test)

# Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy:.4f}")

# Print classification report
print("Classification Report:")
print(classification_report(y_test, y_pred, zero_division=0))

"""Original classes with fewer than 100 records are filtered out to ensure sufficient data for learning and to reduce noise from underrepresented classes."""

# Count the number of records per class
class_counts = white_wine['quality'].value_counts()

# Define a threshold for minimum records per class
threshold = 100

# Filter the dataset to keep only classes with more than the threshold number of records
filtered_white_wine = white_wine[white_wine['quality'].isin(class_counts[class_counts >= threshold].index)]

# Check the new class distribution
print(filtered_white_wine['quality'].value_counts())

# Import necessary libraries
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report
# Define features and target variable
X = filtered_white_wine.drop(columns=["quality"])  # Features
y = filtered_white_wine["quality"]  # Target variable

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=999, stratify=y)

# Initialize the Random Forest model
rf_model = RandomForestClassifier(n_estimators=100, random_state=999)

# Train the model
rf_model.fit(X_train, y_train)

# Make predictions on the test set
y_pred = rf_model.predict(X_test)

# Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy:.4f}")

# Print classification report
print("Classification Report:")
print(classification_report(y_test, y_pred, zero_division=0))

"""Classes are combined to reduce the impact of minority classes without removing data or generating synthetic samples."""

# Combine quality classes
combined_quality_map = {
    3: 1, 4: 1, 5: 2, 6: 3, 7: 3, 8: 4, 9: 4  # Example: Classes 3, 4 -> 1, Classes 5 -> 2, etc.
}

# Apply the mapping to the quality column
filtered_white_wine = filtered_white_wine.copy()
filtered_white_wine['binary_quality'] = filtered_white_wine['quality'].map(combined_quality_map)


# Define features and new target variable
X = filtered_white_wine.drop(columns=["quality", "binary_quality"])  # Features
y = filtered_white_wine["binary_quality"]  # Target variable with combined classes

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=999, stratify=y)

# Initialize the Random Forest model
rf_model = RandomForestClassifier(n_estimators=100, random_state=999)

# Train the model
rf_model.fit(X_train, y_train)

# Make predictions on the test set
y_pred = rf_model.predict(X_test)

# Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy:.4f}")

# Print classification report
print("Classification Report:")
print(classification_report(y_test, y_pred, zero_division=0))

"""# Data Handling Approaches Comparison

| Approach                       | Data Handling                         | Accuracy | Key Advantages                                  | Reasonableness Summary                             |
|-------------------------------|-------------------------------------|----------|------------------------------------------------|---------------------------------------------------|
| 1. SMOTE Oversampling          | Synthetic minority class samples     | ~0.68    | Addresses imbalance via synthetic data         | Limited improvement; synthetic samples may add noise, struggle with very rare classes |
| 2. Class Filtering             | Removed classes with <100 samples    | ~0.70    | Keeps original classes, reduces noise           | More stable; loses very rare classes but retains natural class structure |
| 3. Class Combination           | Merged similar/rare classes to reduce labels | ~0.78    | Simplifies label space, balances class sizes    | Most effective balance of performance and data retention; risks loss of granularity |

---

**Additional notes:**

- All approaches use the same filtered dataset as a base (excluding extremely rare classes where applicable).  
- Accuracy scores based on Random Forest with consistent `random_state=999`.  
- Combining classes improves stability and accuracy but reduces label detail — suitable when fewer, broader categories are acceptable.  
- Filtering classes is a safer choice if preserving original labels is important but requires dropping data from rare classes.  
- SMOTE helps with imbalance but is less effective with extreme class sparsity and may introduce synthetic data artifacts.  

"""